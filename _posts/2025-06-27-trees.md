---
layout: project-post
title: Binary search trees, 2-3-4 trees, and red-black trees
categories: projects
---
<!--Initial post date: 05.20.25-->
<style>
    .spoiler {
        color:black;
        background-color:black;
    }
    .spoiler:hover {
        color:black;
        background-color:white;
        cursor:default;
    }
</style>
"<i>Computer science is no more about computers than astronomy is about telescopes.</i>"
<br>—Edsger Dijkstra
<hr>
<a href="#intro">Introduction</a>
<br><a href="#bst">The Binary Search Tree</a>
<br><a href="#234_trees">B-Trees and 2-3-4 Trees</a>
<br><a href="#rb_trees">Red-black Trees</a>
<br><a href="#conclusion">Conclusion</a>
<br>Thank you to <span title="Lovingly dubbed the Hui Gazette Editor in Chief"><a href="https://zonk.neocities.org" target="_blank">Val</a></span>, <a href="https://twitter.com/aikoheli" target="_blank">Heli</a>, and <a href="https://angelogistics.neocities.org/" target="_blank">Damien</a> for beta-reading!
<hr>
<span id="intro"><b>Introduction</b></span>
<br><br>
When presented with an overwhelming amount of data, how would you change its structure for ease of access and use?
<br><br>
Let me reframe that question in a simpler, more concrete way.
<br><br>
Imagine you have a list of 10,000 words and their definitions in no particular order, and you want to be able to quickly search for any given word to, if it exists in the list, determine its definition. How would you do it?
<br><br>
Your first thought is probably that it'd be awfully tedious to look through every word one by one, so you might want to list them alphabetically and create an index telling you where to look for words that start with A, words that start with B, and so on, like a dictionary. The answer came to you quite naturally, no?
<br><br>
Indexing, like in a dictionary, is just one of many solutions we have for making massive amounts of data reasonable to use. Making data manageable is a fundamental reason why "data structures" exist in computer science. You can think of data structures as ways to organize data for particular uses, with efficient searching and retrieving, which we will continue to discuss, being one such use.
<br><br>
<span id="bst"><b>The Binary Search Tree</b></span>
<br><br>
When searching through a dictionary knowing that your target word starts with, say, the letter E, you can simply search in the section of words that start with E, and ignore everything else. 
In a similar way, many data structures enable you to cut down the scale of your search in order to find your target item more efficiently. This reduction of search scale is the magic of a data structure called the <i>binary search tree</i>. Before we get there, though, let's talk about what a "tree" is in the context of computer science.
<br><br>
A tree is made up of two distinct components—nodes and edges. A node is a bit like a mailbox in the sense that it occasionally has a name (e.g. "Node 1" or "Hui's mailbox"), and often contains some sort of data (e.g. maybe "Node 1" stores the number 7, and "Hui's mailbox" stores a postcard). An edge, on the other hand, is simply a connection between two nodes, enabling you to move between those two nodes in your search. The nodes in a tree must be arranged in a particular way, where there is one node at the "top" layer, called the <i>root</i> node, which branches out to zero or more nodes, which in turn also branch out to zero or more nodes, and so on. Here is how a tree is generally represented in computer science diagrams. Tree diagrams are not often labeled with node names, but I have labeled them in this example to emphasize the point that the values inside the circles are the data the node contains.
<br><img src="/images/for-posts/trees/tree_example.jpg" width="412px">
<br>
When describing trees, there is some terminology to keep in mind. A node branching out to other nodes is called a <i>parent</i> node, and the nodes it branches out to are called its <i>children</i>. Also, a node with no children is called a <i>leaf</i> node ("Branching out" isn't formal terminology, but "leaf" is; funny how that works). So, Node 2 is considered the parent node of nodes 4, 5, and 6, the three of which are leaf nodes because they themselves have no children. Also, note that each node in a tree can strictly only have 1 parent, with the exception of the root node, which has no parent.
<br><br>
It turns out that just constructing some kind of tree doesn't satisfy our searching needs (Hell, even a single node with no edges, which is obviously quite useless for our purposes, is a tree by definition). Similar to how it'd be a terrible idea to put definitions in a dictionary in an arbitrary order, you can't draw nodes and edges based on no particular reasoning to construct a reasonably searchable data structure. Instead, we use strategic structures like the binary search tree. For a tree to be a valid binary search tree, its arrangement of nodes must follow these special rules:
<br><br>
<span id="bst_definition">
    1. Each node can have only up to 2 children.
    <br>2. Each node must contain a value greater than that of its left child, and less than that of its right child.
</span>
<br><br>
Here's an example of a valid binary search tree. Let's use it to see why having the above qualities makes it easy to search for data. Note that downward arrows on the edges in a tree (like in the previous example) are implicit; for searching purposes, we always start from the root and make our way down.
<br><img src="/images/for-posts/trees/bst_example.jpg" width="362px">
<br>Say we're searching for the value 15 in this data. As with all trees, we start from the root node, the one that contains the value 22. Because our target value, 15, is less than 22, we next move to the root node's left child, the node that contains the value 10. Because our target value, 15, is greater than 10, we next move to the current node's right child, the node that contains the value 15. Nice, we found our target value.
<br><br>Notice that we reduced the number of items we needed to search through with each move we made. By deducing that our target value, 15, is less than that of the root node, 22, we could exclude 37, 28, and 49 from our search. Under the assumption that all binary search trees have a roughly equal number of nodes on either side of each "descendant" node, this means that you can cut down the size of your search by up to half with each comparison you make. Though not as impressive on a small scale, you could imagine that this helps immensely when searching through large amounts of data. (This data doesn't strictly have to be numerical, by the way. As long as you can make distinct greater-than or less-than comparisons between some type of data, you can put data of that type into a binary search tree for convenient searching. Relating back to the dictionary example, you could say a word that comes before another word alphabetically is "greater" than that other word and vice versa.)
<br><br>
However, not all binary search trees offer this convenience.
Notice that the following tree is also a valid binary search tree (Refer back to <a href="#bst_definition">the definition</a> if you need to), yet not easy to search because its single file structure forces you to check every node until you find your target value.
<br><img src="/images/for-posts/trees/terrible_tree.jpg" width="342px">
<br>(No left children exist in this tree, so you can only keep going right!)
<br><br>
Evidently, the binary search tree has some extremely flawed marginal cases. Why does this matter? Well, when constructing a binary search tree, we start with a structureless set of data which we assemble piece by piece, with the first item being the root, the next item being its left or right child, and so on and so forth. Basically, when inserting an item into a binary search tree, you pretend as though you are searching for that value, and then you add it at the position you end up in. So, if the order of your input data happens to be ascending or descending, you will end up with the worst possible structure for a binary search tree (i.e., the one above). This is to say that the efficiency of your constructed tree heavily depends on the order of your input. Though the absolute worst case may not be very common, that still won't necessarily prevent your binary search trees from being inefficient in other similarly unbalanced ways, such as having 10 descendant nodes to the left and 100 of them to the right.
<br><br>Therefore, we cannot assume that any arbitrary binary search tree offers us meaningfully improved searching capabilities unless we eliminate the possibility of marginal cases entirely. This is where self-balancing trees, known as <i>B-trees</i>, come in.
<br><br>
<span id="234_trees"><b>B-Trees and 2-3-4 Trees</b></span>
<br><br>As we have previously seen, a binary search tree is most efficient when it has a roughly equal number of nodes on either side of each node, because that enables us to maximize the amount of data we no longer need to consider with each comparison we make. However, with binary search trees alone, we have no way to guarantee the structural balance required for peak efficiency, which is why we look toward a new type of data structure, B-trees.
<br><br>
You can think of B-trees as an extended, self-balancing version of binary search trees. To understand how B-trees work, we will examine the 2-3-4 tree, a specific type of B-tree. Let's first discuss its structure, and then go on to the rules for building a  2-3-4 tree.
<br><br>
First, whereas a binary search tree can contain up to 1 item per node and up to 2 children per node, a 2-3-4 tree can contain up to 3 items per node and up to 4 children per node. Nodes containing 1 item have 2 children, nodes containing 2 items have 3 children, and nodes containing 3 items have 4 children—these nodes are called 2-nodes, 3-nodes, and 4-nodes respectively. If it helps, these points can be generalized to 1) nodes containing <i>n</i> items have <i>n</i>+1 children, and 2) a <i>k</i>-node has <i>k</i> children. Also, the child nodes of any 2, 3, or 4-node may be either 2, 3, or 4-nodes.
<br><br>
Here is an example of a 4-node.
<br><img src="/images/for-posts/trees/4-node.jpg" width="332px">
<br>As seen above, the items in the parent node, 5, 13, and 17, are arranged in ascending order. The leftmost child node contains data smaller than the leftmost parent value, whereas the rightmost child node contains data larger than the rightmost parent value. As for the other nodes, they contain data that is in between the parent values they are situated between (e.g., child node C contains 15, which is between 13 and 17).
All 2, 3, and 4-nodes follow this pattern, which is why each node contains 1 more child than it has items. Also, the small squares under each child node represent empty leaf nodes, aka NIL nodes, to indicate the absence of its children (e.g., child node B contains 2 items and can accomodate 3 children, so there are 3 squares beneath it). Note that these NIL nodes are sometimes not depicted, but still implicitly there.
<br><br>
So, given a bunch of data, how do we build one of these? Like before, we insert items one by one. For each item, we act as though we are searching for it in the 2-3-4 tree, inserting it at one of the bottommost existing nodes. Then, we handle any overflows; that is, when an insertion causes a node to contain 4 items rather than the maximum of 3. There's a simple intuition as to why we've devised this process like this.
Say you're adding an entry to an indexed database. You wouldn't first update the index, you'd add the new entry where it belongs among the other data, and then update the index if necessary.
Similar to an indexed database, the leaf nodes of a 2-3-4 tree contain the bulk of its data, whereas the layers  above act as a tiered index directing you toward that data. So, in a similar fashion, we always insert data into the bottommost existing nodes, before then adjusting the higher, "index" nodes if necessary. To start, though, we create the root node and begin inserting items there. Let's walk through an example to see this process in action.
<br><br>
Suppose we want to insert the numbers 61, 50, 47, 28, 13, 42, 75, 19, 10, 54, 96, and 88 into a 2-3-4 tree, in that order. We begin, of course, by creating the root node and inserting numbers there.
<br><img src="/images/for-posts/trees/234_1.jpg" width="180">
<br>Next, we need to insert 28. But when we do, we'll have 4 items in one node, violating our specified maximum of 3. What now?
<br><img src="/images/for-posts/trees/234_2.jpg" width="255px">
<br>The answer is that we do this:
<br><img src="/images/for-posts/trees/234_3.jpg" width="270px">
<br>This is called a 1-2 split, because we end up with 1 item in the node to the left and 2 items in the node to the right. We split like this so there are roughly the same number of items on either side. So, a 2-1 split would've been about the same thing, but I've decided to stick only with 1-2 for consistency. I should now mention that the maximum capacity of a node is an arbitrary specification. Instead of a maximum of 3, you could specify, say, a maximum of 99, and split with 50 items to the left and 49 items to the right. That would be called a B-tree of degree 100, because a node with the maximum number of items, 99, would be able to accomodate 100 children. Similarly, a 2-3-4 tree is really just a B-tree of degree 4. So, though a 1-2 or 2-1 split seems pretty uneven, a difference of 1 becomes increasingly insignificant the more data you allow in a single node.
<br><br>
Let's keep adding to our 2-3-4 tree to see how the split operation works on an increasingly populated tree. As a reminder, we still have 13, 42, 75, 19, 10, 54, 96, and 88 left.
<br><img src="/images/for-posts/trees/234_4.jpg" width="362px">
<br>When we insert 19, we violate the maximum of 3 again, so we perform the same splitting operation as before.
<br><img src="/images/for-posts/trees/234_5.jpg" width="452px">
<br>Now to insert 10, 54, 96, and 88. Inserting 10, 54, and 96 is pretty standard:
<br><img src="/images/for-posts/trees/234_6.jpg" width="462px">
<br>But when we attempt to insert 88, we get a double violation. How do we fix this?
<br><img src="/images/for-posts/trees/234_7.jpg" width="462px">
<br>The answer is simple; we also do a 1-2 split on the violated node! Only this time, we shift the node's children along with the split.
<br><img src="/images/for-posts/trees/234_8.jpg" width="462px">
<br>As per the standard structure of a 2-3-4 tree, the node with 1 item ends up with 2 children, and the node with 2 items ends up with 3 children. And with that, we have finished making our tree.
<br><br>
Notice that the 2-3-4 tree we've constructed is very balanced—the right side of the tree is perfectly level with the left side. This is no coincidence. Because the splitting operation we use to increase the height of the tree never adds to one side of the tree without also adding to the other, 2-3-4 trees always have the same number of layers either side, meaning it is impossible to construct the marginal case we reviewed in the section about binary search trees. You can confirm this by using <a href="https://www.cs.usfca.edu/~galles/visualization/BTree.html" target="_blank">this B-tree visualization program</a>; select "Max. Degree = 4" to simulate the 2-3-4 tree.
<br><br>
So, we've solved the problem, haven't we? You could say so. But computer scientists didn't stop here.
<br><br>
We didn't technically fix the issue with the binary search tree, we simply created a new type of tree to address it. Is there a way to capitalize on the balancing mechanism of B-trees while also retaining the structure of a valid binary search tree? The answer is yes, and the <i>red-black tree</i> is proof.
<br><br>
<span id="rb_trees"><b>Red-black Trees</b></span>
<br><br>
The red-black tree follows the same structural rules as the binary search tree, yet rearranges itself in a way parallel to the way a 2-3-4 tree does.
According to Wikipedia, the properties of a red-black tree are as follows:
<br><br>"In addition to the requirements imposed on a binary search tree the following must be satisfied by a red–black tree:
<br>1. Every node is either red or black.
<br>2. All null nodes are considered black.
<br>3. A red node does not have a red child.
<br>4. Every path from a given node to any of its leaf nodes goes through the same number of black nodes.
<br>5. (Conclusion) If a node N has exactly one child, the child must be red. If the child were black, its leaves would sit at a different black depth than N's null node (which is considered black by rule 2), violating requirement 4."
<br><br>When you look up "red-black tree" online, you're likely to encounter definitions more or less like this. Though you might understand the properties in themselves, how do they eliminate the issue of unbalanced trees, and how are they related to 2-3-4 trees? To understand these questions, we'll look at an alternate but equivalent representation of red-black trees, which is edge-based instead of node-based (i.e., edges are red or black instead of nodes, whose colors don't have any particular significance).
<br><br>
Here is the 2-3-4 tree we previously constructed and an edge-based red-black tree that is its exact equivalent. Can you infer what the correlation is? Hint: <span class="spoiler">Red is glue.</span>
<br><img src="/images/for-posts/trees/rb_1.jpg" width="450px">
<br>If you guessed that a red edge between two nodes in the red-black tree indicates that the data in those nodes are in the same node in the 2-3-4 tree, you're absolutely correct.
As pictured above, 2-nodes are represented the same way in the red-black tree as they are in the 2-3-4 tree, only their children must also be 2-nodes, because 3-nodes and 4-nodes do not exist in binary search trees or red-black trees. On the other hand, 3-nodes can be represented either of these two ways: 
<br><img src="/images/for-posts/trees/rb_3-node.jpg" width="440px">
<br>And 4-nodes are represented like this:
<br><img src="/images/for-posts/trees/rb_4-node.jpg" width="320px">
<br><br>So, how does this work with the more conventional, node-based red-black tree? It's simple: Every node with a red edge just above it in the edge-based representation is red, whereas the rest are black.
<br><img src="/images/for-posts/trees/rb_evn.jpg" width="390px">
<br>Notice that the above red-black tree follows all the rules for red-black trees as described on Wikipedia:
<br>1. <u>Yes</u>, every node is either red or black.
<br>2. <u>Yes</u>, null aka NIL nodes are implicit, so we can simply consider them black.
<br>3. <u>Yes</u>, no red node has a red child.
<br>4. <u>Yes</u>, every path from a given node to any of its leaf nodes goes through the same number of black nodes. We can tell because every path from the root to a leaf node contains exactly 3 black nodes.
<br>5. <u>Yes</u>, every node with exactly one child has a red child; see the nodes containing 13, 28, and 88.
<br><br>Despite edge-based red-black trees being more human intuitive, we often use node-based red-black trees in practical implementations because edges are not often defined as structures that carry information in itself, whereas nodes always are. So, programmers use 1 bit in each node's data to indicate whether that node is red or black (e.g. 0 = black, 1 = red, or vice versa). Also, keep in mind that when someone talks about red-black trees, they are probably referring to the node-based version unless otherwise specified. 
<br><br>Okay, so what about insertion? It turns out that insertion in 2-3-4 trees and red-black trees appear quite different despite having very significant conceptual overlap. 
Let's stick with the more intuitive edge-based red-black tree to compare and contrast the two insertion processes and better understand how the red-black tree self-balances.
Similar to insertion in binary search trees and 2-3-4 trees, we first act as though we are searching for our item-to-insert in the red-black tree, and then put it at the position we end up in. Next, we color the edge connecting the new node red, and handle any violations through rotations or recolorings.
In an edge-based red-black tree, a violation occurs if:
<br><br>1. There are 2 consecutive red edges along any path from the root to a leaf node.
<br>2. There is an unequal number of black edges along the paths from the root to a leaf node.
<br>(It follows, then, that the rules of an edge-based red-black tree are these two statements but inverted.)
<br><br>As you might've inferred, coloring the new edge red and then considering any resulting violations is the equivalent of what we did when inserting data into a 2-3-4 tree (Recall that a red edge indicates that data is "in the same node"). Moreover, the above two conditions for violation are equivalent to rules 3 and 4 from the Wikipedia definition of a node-based red-black tree.
<br><br>
Because of these particular constraints, we obviously can't start inserting data the way we did in the 2-3-4 tree.
<br><img src="/images/for-posts/trees/no_in_rb.jpg" width="235px">
<br>Instead, it looks like this:
<br><img src="/images/for-posts/trees/rb_ins_1.jpg" width="305px">
<br>Rather than inserting data where it needs to be within a node, we insert node-by-node and rotate to maintain balance.
<br>As for our first "overflow" upon adding a 4th item, it looks like this:
<br><img src="/images/for-posts/trees/rb_ins_2.jpg" width="270px">
<br>Here, inserting 28 causes a violation because it makes 2 consecutive red edges in the tree, so we color the edge preceding it black to fix that. Then, we color the edge on the right side of the tree black to maintain an equal number of black edges per path. After changing those two edges from red to black, there are no violations present. Though we haven't technically changed anything structurally, the resulting red-black tree is the equivalent of this 2-3-4 tree, which is the result of doing a 2-1 split:
<br><img src="/images/for-posts/trees/rb_ins2_eq.jpg" width="255px">
<br>See the conceptual overlap?
<br><br>You're probably well aware by now that red-black trees are not always perfectly balanced. Even though the two trees above are "equivalent", red-black trees requiring one node per piece of data means its left side may not be level with the right. By design, red-black trees allow one side of the tree to be at most twice as long as the other. Since there must be 1) no consecutive 2 red edges along any path, and 2) the same number of black edges along every path, the theoretical worst case red-black tree is a tree with <i>n</i> black edges along one path, and <i>n</i> black edges plus <i>n</i> red edges each spaced apart by one black edge along the other. So, while not necessarily guaranteeing perfect balance, red-black trees still effectively prevent the egregious cases we previously discussed.
<br><br>
<span id="conclusion"><b>Conclusion</b></span>
<br><br>
Binary search trees, 2-3-4 trees (or more generally, B-trees), and red-black trees are just a few of many solutions computer scientists have devised to make large amounts of data manageable. By elaborating on the fundamental concepts and details leading into my explanations of these data structures, I hope this article helped you learn something new regardless of how much you knew going in.
<br><br>
Because this article is strongly focused on helping readers gain an intuitive conceptual understanding, two important and relevant topics I did not cover are deletion in the types of trees, as well as methods for practical implementation. Nevertheless, this article should equip you to learn about these topics elsewhere if you are interested.
<br><br>
Since I've hardly written anything like this and hope to write similar articles about other topics in the future, feedback is much appreciated. If you're willing, let me know what you thought of this article by <a href="mailto:abacist@disroot.org">sending me an email</a> or <a href="https://abacist.atabook.org">writing in my guestbook</a>.